{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Big Data and Google Cloud\n",
    "\n",
    "UIC CS 418, Spring 2020\n",
    "\n",
    "_According to the **Academic Integrity Policy** of this course, all work submitted for grading must be done individually, unless otherwise specified. While we encourage you to talk to your peers and learn from them, this interaction must be superficial with regards to all work submitted for grading. This means you cannot work in teams, you cannot work side-by-side, you cannot submit someone elseâ€™s work (partial or complete) as your own. In particular, note that you are guilty of academic dishonesty if you extend or receive any kind of unauthorized assistance. Absolutely no transfer of program code between students is permitted (paper or electronic), and you may not solicit code from family, friends, or online forums. Other examples of academic dishonesty include emailing your program to another student, copying-pasting code from the internet, working in a group on a homework assignment, and allowing a tutor, TA, or another individual to write an answer for you. Academic dishonesty is unacceptable, and penalties range from failure to expulsion from the university; cases are handled via the official student conduct process described at https://dos.uic.edu/conductforstudents.shtml._\n",
    "\n",
    "This homework is an individual assignment for all graduate students. Undergraduate students are allowed to work in pairs and submit one homework assignment per pair. There will be no extra credit given to undergraduate students who choose to work alone. The pairs of students who choose to work together and submit one homework assignment together still need to abide by the Academic Integrity Policy and not share or receive help from others (except each other).\n",
    "\n",
    "\n",
    "## Due Date\n",
    "\n",
    "This assignment is due at 11:59pm on April 23rd, 2020.\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "You need to complete all code and answer all questions denoted by **Q#** in this notebook. When you are done, you should export **`hw4.ipynb`** with your answers as a PDF file, upload that file `hw4.pdf` to *Homework 4 - written* on Gradescope, tagging each question. \n",
    "\n",
    "Select your submission subdirectory `sub` and your completed Jupyter notebook (`hw4.ipynb` file) and zip it. As a result your `sub` subdirectory and `hw4.ipynb` file should be in the root of your zip file. Upload this zip file to *Homework 4 - code* on Gradescope. \n",
    "\n",
    "For undergraduate students who work in a team of two, only one student needs to submit the homework and just tag the other student on Gradescope.\n",
    "\n",
    "**Keep an eye out for the following icons:**\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./icons/edit.png\" width=\"20px\" heigh=\"20px\" align=\"left\">  Specifies that you need to add something to the notebook in order to get credit.\n",
    "\n",
    "\n",
    "<img src=\"./icons/save.png\" width=\"20px\" height=\"20px\" align=\"left\">  Specifies that you need to save something to `sub` (submission) subdirectory or verify the existence of some file in `sub` to get credit.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\">  Warnings to avoid common mistakes and pitfalls. Pay special attention to these.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Autograding\n",
    "\n",
    "Q3 and Q4 will be graded based on your PDF submissions, the rest of the questions will be graded using an Autograder. All parts of Homework 4 are graded based on correctness, **not** based on completion.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before We Start\n",
    "\n",
    "Before we dive into the assignment, we need to install the following python packages:\n",
    "\n",
    "- seaborn*\n",
    "- matplotlib*\n",
    "- nltk*\n",
    "- sklearn*\n",
    "- pandas*\n",
    "- numpy*\n",
    "- google-cloud\n",
    "- google-cloud-storage\n",
    "- google-cloud-bigquery[pandas]\n",
    "\n",
    "Packages marked with * should be pre-installed in a conda environment. You may install missing packages one at a time using `pip` command or you can run the following shell command to install them all at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ./requirements.txt > pip-log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\"> If you are installing using the shell command listed above then pay attention to any warnings and check the logs `pip-log.txt`.\n",
    "\n",
    "Now, we are ready to import all dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to ensure that we have required nltk packages installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/pwang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/pwang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/pwang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have everything we need, we can dive into the homework itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud Services\n",
    "\n",
    "Most real world systems in the modern world generate copious amount of data (billions of records) and in order to process this data and to enable reasoning through statistical models, we need high computational power and memory which is not available in most personal machiens.\n",
    "\n",
    "One solution is to build and maintain your own clusters of high-end machines which has a massive overhead and cost associated with it, which is definitely not suitable for individuals or small teams. Hence, most individuals and companies rely on cloud service providers for a range of services, including but not limited to storage, processing, querying and visualizing large datasets.\n",
    "\n",
    "In this homework, we will use [Google Cloud](https://cloud.google.com/) to query and visualize data and we will train and deploy a machine learning model in the cloud. The rest of the homework is divided in 4 parts.\n",
    "\n",
    "**Part 0:** Setup: Initial configurations\n",
    "\n",
    "**Part 1:** Big Query: A database for Big Data\n",
    "\n",
    "**Part 2:** Data Studio: A visualization tool for Big Data\n",
    "\n",
    "**Part 3:** Cloud ML: Machine Learning with Big Data\n",
    "\n",
    "These services can save a lot of time and make your life easier when handling a large amount of data which cannot be processed using a single machine. So let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Setup\n",
    "\n",
    "\n",
    "### Setup Billing\n",
    "Follow this [link](https://google.secure.force.com/GCPEDU?cid=9WzDrig1uX2HIVG8kMMLUnSUJwvzPEDNu8l4yO3eLPNalOh%2B8RGwKqJwJglg1iDV) to claim \\\\$50 credit using your @uic.edu email. This homework can be completed while utilizing less than \\\\$5. Fill up the details in following form and it will send you an email to verify your account.\n",
    "\n",
    "<img src='./screens/credit/00-first-form.png'>\n",
    "\n",
    "After you have received your verification email (shown below), follow the link in email to request a coupon \n",
    "\n",
    "<img src='./screens/credit/01-verification-email.png'>\n",
    "\n",
    "You will receive a coupon in email as shown below, follow the link to redeem it.\n",
    "\n",
    "<img src='./screens/credit/02-coupon.png'>\n",
    "\n",
    "Once you accept and add credit to your account, you will be sent to the following page.\n",
    "\n",
    "<img src='./screens/credit/03-verify-credit.png'>\n",
    "\n",
    "Verify that \\$50 have been added to your billing account \"CS 418 Introduction to Data Science\"\n",
    "\n",
    "\n",
    "### Create Project\n",
    "\n",
    "Google Cloud provides a common interface to manage and monitor all projects, this is called Google Cloud Console. First, you need to log into [Google Cloud Console](https://console.cloud.google.com/) using your university ID.\n",
    "\n",
    "After logging in, you will land on a welcome page which, assuming you have not setup a project before, lists some popular services and tutorials shown below:\n",
    "\n",
    "<img src='./screens/misc/welcome.png'>\n",
    "\n",
    "\n",
    "Click on the dropdown \"Select a Project\" indicated by the red arrow in the image above and you will be taken to this menu:\n",
    "\n",
    "<img src='./screens/misc/00-select-project.png'>\n",
    "\n",
    "Click on the New Project button as shown above and it will take you to project form:\n",
    "\n",
    "<img src=\"./screens/misc/01-name-project.png\">\n",
    "\n",
    "\n",
    "Name your project `homework4` and click *Create* button. And just as easily, your project is set up.\n",
    "\n",
    "All the Google Cloud services are offered within the context of a project and once the project is created, you will be redirected to project dashboard (or Google Cloud Console) where you can add services to the project, monitor activity and manage billing etc.\n",
    "\n",
    "<img src=\"./screens/misc/02-project-dashboard.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link Billing Account\n",
    "\n",
    "Before, we proceed, we need to link a billing account to our project or verify that the previously created billing account is linked to our project automatically. First, ensure that the correct project is selected, then open the left sidebar menu using the button on top-left and select Billing.\n",
    "\n",
    "<img src=\"./screens/credit/00-link-billing.png\">\n",
    "\n",
    "This will take you to a screen which specifies the billing account linked to your project. Verify that the linked account is \"CS 418 Introduction to Data Science Jan 2020\".\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\"> <font color='red'> You might have multipled billing accounts, as shown in the image below, in that case, make sure that the correct account is linked to avoid any unnecessary cost.</font>\n",
    "\n",
    "<img src=\"./screens/credit/01-multiple-billing.png\">\n",
    "\n",
    "Now that our project is setup, let's add a service to it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Bucket\n",
    "\n",
    "A bucket is just a storage container, it serves the same purpose as a cloud storage service (e.g. *Google Drive*) but it is more suitable for programmatic access and integration with other services as we will see later. Buckets are managed through a service *Cloud Storage* and we can add a bucket to our project through the sidebar (topleft menu) as shown in the following image:\n",
    "\n",
    "<img src=\"./screens/bucket/00-select-storage.png\" />\n",
    "\n",
    "Before you add any service, make sure that your active project is the one you just created. Then you can select the sidebar menu by clicking on the top-left button and add services from the menu.\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\">   <font color='red'>Adding services may incur a high cost so only add what you need.</font>\n",
    "\n",
    "Selecting *Browse* will take you to the following screen:\n",
    "\n",
    "<img src=\"./screens/bucket/01-select-create.png\" />\n",
    "\n",
    "Click on the *Create Bucket* button as shown above and you will be taken to the following form:\n",
    "\n",
    "<img src=\"./screens/bucket/02-bucket-form-name-region.png\"/>\n",
    "<img width=\"600\" height=\"800\" src=\"./screens/bucket/03-bucket-form-rest.png\"/>\n",
    "\n",
    "\n",
    "Fill out the form as shown above (use the provided links to learn more about these fields), however you need to enter a unique bucket name and press create. Now you should be able to view your bucket in a file browser like environment. We will come back to this later, first, let's set up authentication first so that we can communicate with Google Cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Authentication\n",
    "\n",
    "We will follow the simple steps listed in this tutorial ([Authentication tutorial](https://cloud.google.com/docs/authentication/getting-started)) to setup authentication.\n",
    "\n",
    "First, we need to setup a service account, open the tutorial and click on *GO TO THE CREATE SERVICE ACCOUNT KEY PAGE* button and fill up the form as shown below. When the account is created, you will be prompted to download a JSON key, save it in homework root directory as `homework4-key.json`.\n",
    "\n",
    "<img src=\"./screens/misc/auth-account.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now setup an enviornment variable `GOOGLE_APPLICATION_CREDENTIALS` using the following python command, this enables the google cloud API to use the JSON key while making requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'homework4-key.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can test our setup by making an authenticated API call to list buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def list_buckets():\n",
    "    storage_client = storage.Client()\n",
    "    # Make an authenticated API request\n",
    "    buckets = list(storage_client.list_buckets())\n",
    "    print(buckets)\n",
    "\n",
    "    \n",
    "list_buckets()\n",
    "# The expected output of this command should contain the name of your bucket\n",
    "# [<Bucket: [YOUR BUCKET NAME]>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this JSON key, we can now communicate with server without having to explicitly log in.\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\">   `homework4-key.json` contains sensitive information that you should not share with anyone. Do not submit this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: BigQuery\n",
    "\n",
    "BigQuery is a large scale database which can process multiple GBs of data (e.g. all crime records in Chicago since 2001) within just a few seconds and provides the capacity to store and retrieve structured information which can either be used by your application or another cloud service. You can read more about it on the [project page](https://cloud.google.com/bigquery/).\n",
    "\n",
    "One of the key strenghts of BigQuery is that it supports Structured Query Language (SQL) which is the de facto query language for Relational Databases in which data is stored in tabular form (just like pandas! but not in-memory). This enables the developers to query large databases without having to learn a new language, however, if you are not familiar with SQL or a simple refresher course then you can get a quick introduction at [W3 Schools](https://www.w3schools.com/sql/). \n",
    "\n",
    "Let's first go to BigQuery interface and explore some large scale public datasets. From the [project dashboard](https://console.cloud.google.com) select BigQuery from the sidebar:\n",
    "\n",
    "<img src=\"./screens/bquery/01-select-bigquery.png\" />\n",
    "\n",
    "\n",
    "This will take you to the BigQuery interface. For this homework, we will be using publicly available chicago crime data which encompasses crime records in Chicago since 2001. It is already structured, cleaned and maintained by Google and we can just directly query it, however we can also add our own datasets to BigQuery if needed.\n",
    "\n",
    "<img src=\"./screens/bquery/02-select-table.png\" />\n",
    "\n",
    "Browse and select the table from the sidebar as shown in the figure and you will be able to see all the fields in the table and their description. You can also select *Preview* to show a subset of the dataset.\n",
    "\n",
    "Let's run this simple query using the Query Editor\n",
    "\n",
    "```\n",
    "#standardSQL\n",
    "\n",
    "SELECT * FROM `bigquery-public-data.chicago_crime.crime`\n",
    "WHERE primary_type = \"THEFT\"\n",
    "LIMIT 100;\n",
    "```\n",
    "\n",
    "This query will select the top 100 crime records of type theft form the crime dataset.\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\"> **\\`** or backtick is a character which is not the same as an apostrophe or double quotes and is often used in SQL.\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\"> You always need to enter the prefix of the table or the dataset identifier *bigquery-public-data.chicago_crime* to access *crime* table.\n",
    "\n",
    "\n",
    "<img src=\"./screens/bquery/03-query-results.png\" />\n",
    "\n",
    "In the figure above, you can see the results listed as a table. The Query editor shows you how much data it is going to process when the query runs so that you may estimate the cost. In this case, we are running this query on 1.3 GB of data within a few seconds. You may also export results in CSV format or explore them in Data Studio (a visualization portal that we will discuss later). Just like `pandas`, you may also perform a Group By operation and order the results using the following query:\n",
    "\n",
    "```\n",
    "SELECT primary_type, COUNT(*) as count FROM `bigquery-public-data.chicago_crime.crime`\n",
    "GROUP BY primary_type\n",
    "ORDER BY count;\n",
    "```\n",
    "\n",
    "The results are as follows:\n",
    "\n",
    "<img src=\"./screens/bquery/04-query-group.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 (20%)\n",
    "\n",
    "<img src=\"./icons/edit.png\" width=\"20px\" height=\"20px\" align=\"left\"> Write an SQL query to extract the number of arrests/no-arrests per primary type for domestic-related cases, exclude \"OTHER OFFENSE\" and all non-criminal types, only consider records until the end of year 2019 and sort the results in ascending order by primary type and arrest.\n",
    "\n",
    "<img src=\"./icons/save.png\" width=\"20px\" height=\"20px\" align=\"left\"> Take a screenshot (full screen, png format), rename it as `q1-results.png` and store it in `sub` subdirectory.\n",
    "\n",
    "<img src=\"./icons/edit.png\" width=\"20px\" height=\"20px\" align=\"left\"> Include screenshot in the notebook by editing the following markdown (if needed). It shows as a broken image by default if screenshot is unavailable.\n",
    "\n",
    "<img src=\"./sub/q1-results.png\" align=\"center\"/>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\"><font color=\"red\"> Refresh your browser if it does not appear right away.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `pandas` and SQL operate on tables and offer similar functionality, we can transfer SQL results to a `pandas` data frame for further processing. One arduous way to do it is to first export results from BigQuery interface as a CSV file and load it in pandas, but there is a much better and convenient method available through the `bigquery` magic command. The following statement will run a query on the server and store results in the specified `df` dataframe, while running it shows the time elapsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery df\n",
    "\n",
    "-- This is an SQL comment\n",
    "\n",
    "\n",
    "SELECT * FROM `bigquery-public-data.chicago_crime.crime`\n",
    "WHERE primary_type = \"THEFT\"\n",
    "LIMIT 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now print a subset of these results using this familiar method\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./icons/edit.png\" width=\"20px\" height=\"20px\" align=\"left\"> Copy the query you wrote earlier to retrieve the number of arrests per each primary type for domestic-related cases in the following cell.\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\"> This is important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery q1df\n",
    "\n",
    "-- Copy your SQL query below this line. (Make sure you rename the count aggregation as \"counts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now view a sample of our results\n",
    "# primary_type, arrest, counts\n",
    "q1df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Using the same dataframe, we can create a plot showing the fraction of arrests for each crime category\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Normalizing results\n",
    "q1df[\"crime_freq\"] = q1df.groupby(\"primary_type\")[\"counts\"].transform(lambda x:x.sum())\n",
    "q1df[\"percentage\"] = q1df[\"counts\"].mul(100.0).divide(q1df[\"crime_freq\"])\n",
    "q1df[\"primary_type\"] = q1df[\"primary_type\"].apply(lambda x: x.capitalize())\n",
    "norm_df = q1df[q1df[\"arrest\"] == True]\n",
    "norm_df = norm_df.sort_values(by=\"crime_freq\", ascending=False)\n",
    "# Standard sns barplot\n",
    "ax = sns.barplot(x=\"primary_type\", y=\"percentage\", data=norm_df, color=\"b\")\n",
    "# Adding context to the plot\n",
    "plt.xticks(rotation=90, fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel(\"Crime Type (Descending order of crime frequency)\", fontsize=15)\n",
    "plt.ylabel(\"Percentage of Arrests\", fontsize=15)\n",
    "plt.title(\"Arrests in Domestic-related Crime types in Chicago from 2001 to 2019\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your graph should look like the graph below:\n",
    "\n",
    "<img src=\"./graph.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can save this file in our submission subdirectory\n",
    "out_filepath = os.path.join(\"sub\", \"crime_arrests.csv\")\n",
    "df.to_csv(out_filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./icons/save.png\" width=\"20px\" height=\"20px\" align=\"left\"> Check your `sub` (submission) subdirectory to verify that you have a file named `crime_arrests.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 (20%)\n",
    "\n",
    "<img src=\"./icons/edit.png\" width=\"20px\" height=\"20px\" align=\"left\"> For each location category (column `location_description`) find the most frequent crime type. Your final results (in the CSV) should have three columns `location_description`, `primary_type`, `counts`. Where `location_description` is the category for different locations such as streets, residence, etc., `primary_type` is the most frequent crime on that location category and `counts` is simply the number of records associated with that crime. Exclude \"OTHER OFFENSE\", all non-criminal types and null values of `location-description`. Only consider records until the end of year 2019 and sort results in ascending order by `location_description`.\n",
    "\n",
    "It would be simpler to first extract relevant counts using SQL and then find the most frequent one for each block using `pandas` but some SQL experts might be able to do it in a single SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery q2df\n",
    "\n",
    "#standardSQL\n",
    "-- Write your query below this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Write python code here for post-processing with pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see a subset of results\n",
    "q2df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can save this file in our submission subdirectory\n",
    "out_filepath = os.path.join(\"sub\", \"freq_crime.csv\")\n",
    "q2df.to_csv(out_filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./icons/save.png\" width=\"20px\" height=\"20px\" align=\"left\"> Check your `sub` (submission) subdirectory to verify that you have a file named `freq_crime.csv` following the format specified above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Google Data Studio\n",
    "\n",
    "In this part, we will explore a  tool called [Google Data Studio](https://datastudio.google.com) which allows us to create visualizations using large scale datasets. This tool can retrieve data from many sources (including BigQuery tables!) and provide a simple interface to create dynamic and sophisticated visualizations. Let's get started!\n",
    "\n",
    "\n",
    "## Q3 (20%)\n",
    "\n",
    "Credit will be awarded based on completion of this tutorial. We will try to replicate the graph we created earlier in **Q1**.\n",
    "\n",
    "First, we need to create **a new (blank) report**, use your last name as the name of report with a suffix `-q3`, if your are working in group then the report name should be of the format `lastname1-lastname2-q3`.\n",
    "\n",
    "\n",
    "You will be directed to the following screen after report is created. Select BigQuery as the data source.\n",
    "<img src=\"./screens/dstudio/00-select-bquery-data.png\"/>\n",
    "\n",
    "\n",
    "Select the crime table from chicago_crime public dataset and click *Add* button.\n",
    "\n",
    "<img src=\"./screens/dstudio/01-select-chicago-crime-data.png\"/>\n",
    "\n",
    "\n",
    "This will take you to add data interface with a default chart. Delete the chart and rename the report as instructed earlier. \n",
    "\n",
    "<img src=\"./screens/dstudio/02-rename-report.png\"/>\n",
    "\n",
    "\n",
    "Now that we have a data source added to the report, we can use it to create charts. Click *Add a chart* dropdown menu and select *100% stacked column chart* from it as shown below.\n",
    "\n",
    "\n",
    "<img src=\"./screens/dstudio/03-add-chart.png\"/>\n",
    "\n",
    "Once, the chart is added, you can click on the chart to view Chart configurations in the right sidebar. Under *DATA*, we can configure fields and under *STYLE*, we can configure appearances of the chart. Setup the fields as shown in the figure below:\n",
    "\n",
    "<img src=\"./screens/dstudio/04-stacked-100-chart.png\"/>\n",
    "\n",
    "Here, *Dimension* is the variable on the x-axis, *Breakdown Dimension* is the same as *hue* in seaborn. *Metric* specifies how you are going to aggregate records and *Sort* is just sorting on the x-axis. Something seems missing in this graph (can you spot it?)\n",
    "\n",
    "Now, we need to use correct filters. Click on *ADD A FILTER* under *DATA* in the right sidebar and setup the appropriate filters as shown below:\n",
    "\n",
    "<img src=\"./screens/dstudio/05-filter.png\"/>\n",
    "\n",
    "\n",
    "The graph we created earlier had a lot more bars than this one and this is because, by default, it only shows a subset of data. Let us fix that under the *STYLE* menu by setting *Bars* to a large number (200).\n",
    "\n",
    "\n",
    "<img height=\"400\" width=\"250\" src=\"./screens/dstudio/06-chart-style-bars.png\"/>\n",
    "\n",
    "\n",
    "That should fix the bars, but we also want to label our axes. We can also do that in the *STYLE* menu by checking these boxes:\n",
    "\n",
    "<img height=\"400\" width=\"250\" src=\"./screens/dstudio/06-chart-style-labels.png\"/>\n",
    "\n",
    "\n",
    "Now we can see all crime types and the axis labels but the labels are not very readable, we can change them by clicking the relevant areas highlighted by red arrows in the following figure.\n",
    "\n",
    "\n",
    "<img src=\"./screens/dstudio/07-rename-labels.png\"/>\n",
    "\n",
    "\n",
    "Our graph is now almost ready, we can add title using *Text* tool from the toolbar and we can drag inner boundary (signified by orange line in the figure below) to make our x-axis labels more visible. We can drag the external boundaries to stretch the graph and voila, with just a few clicks, we managed to create this graph from a large dataset.\n",
    "\n",
    "<img src=\"./screens/dstudio/08-title-context.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "\n",
    "<img src=\"./icons/save.png\" width=\"20px\" height=\"20px\" align=\"left\"> Take a screenshot (full screen, PNG format), rename it as `q3-tutorial.png` and store it in `sub` subdirectory.\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\"> Make sure that the report name and graph is visible in the screenshot, no credit will be awarded otherwise.\n",
    "\n",
    "<img src=\"./icons/edit.png\" width=\"20px\" height=\"20px\" align=\"left\"> Include screenshot in the notebook by editing the following markdown (if needed). It shows as a broken image by default if screenshot is unavailable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./sub/q3-tutorial.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "The report is now ready to be shared. Click the *Share* icon and get a Shareable link similar to Google docs:\n",
    "\n",
    "<img src=\"./screens/dstudio/09-share.png\" width=\"400\" height=\"200\" />\n",
    "\n",
    "\n",
    "<img src=\"./icons/edit.png\" width=\"20px\" height=\"20px\" align=\"left\"> Copy the sharable link to report from the previous step into the box below. Anyone within the University of Illinois should be able to view this report without having to explicity request access.\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\"> No credit will be awarded without this link. Test this link in incognito mode of the browser.\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\"> DO NOT change/delete the report after submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ YOUR LINK HERE ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 (20%)\n",
    "\n",
    "This question is more open ended and similar to Q2.2-Q2.4 from *HW2*. It will be graded in the same way, based on the completeness of the report produced and the insights you gained from it.  We will be using the publicly avaialable chicago crimes dataset for this task and you are only required to produce a single graph.\n",
    "\n",
    "Be sure to consider transformations, subsets, correlations, reference markers, and lines/curves-of-best-fit (as covered in Chapter 6 of PTDS) to reveal the relationship that you are wanting to learn more about.  Also be sure to make plots that are appropriate for the variable types.  For completeness, be explicit about any assumptions you make in your analysis.  An exemplary plot will have:\n",
    "\n",
    "* A title\n",
    "* Labelled and appropriately scaled axes\n",
    "* A legend, if applicable\n",
    "* A carefully selected color scheme\n",
    "* A main point, accentuated through design choices\n",
    "\n",
    "\n",
    "First, same as **Q3**, you need to create a new (blank) report, use your last name as the name of report with a suffix `-q4`, if your are working in group then the report name should be of the format `lastname1-lastname2-q4`.\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\"> No credit will be awarded if the report name is wrong.\n",
    "\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\"> Do not use the same report that you used in **Q3**\n",
    "\n",
    "### Submission\n",
    "\n",
    "\n",
    "<img src=\"./icons/edit.png\" width=\"20px\" height=\"20px\" align=\"left\"> Write your main takeaway/hypothesis 5-15 words in the following cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ YOUR MAIN TAKEAWAY HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./icons/edit.png\" width=\"20px\" height=\"20px\" align=\"left\"> Write a description 100-150 words following cell explaining your assumptions and what you have found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[YOUR DESCRIPTION HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./icons/save.png\" width=\"20px\" height=\"20px\" align=\"left\"> Take a screenshot (full screen, PNG format), rename it as `q4-report.png` and store it in `sub` subdirectory.\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\"> Make sure that the report name and graph is visible in the screenshot, no credit will be awarded otherwise.\n",
    "\n",
    "<img src=\"./icons/edit.png\" width=\"20px\" height=\"20px\" align=\"left\"> Include screenshot in the notebook by editing the following markdown (if needed). It shows as a broken image by default if screenshot is unavailable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./sub/q4-report.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "<img src=\"./icons/edit.png\" width=\"20px\" height=\"20px\" align=\"left\"> Create a sharable link to this report using the process shown in **Q3** and add it in the next cell. Anyone within the university should be able to view this report without having to explicity request access.\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\"> No credit will be awarded without this link. Test this link in incognito mode of the browser.\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\"> DO NOT change/delete the report after submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ YOUR LINK HERE ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Google Cloud ML Engine\n",
    "\n",
    "In this part, we will use [Google Cloud ML Engine](https://cloud.google.com/ml-engine/) to train and deploy a simple machine machine learning model. Training models from large datasets is a particularly tedious task, especially if it involves hyper-parameter tuning and training may take days to complete and require high computational powers and multiple CPUs/GPUs/TPUs. Google Cloud ML Engine addressess all these problems. Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 (20%)\n",
    "\n",
    "Credit will be awarded based on completion of this tutorial. Same, as before, we will train our model using tweets dataset from *HW3* and make predictions on the test set, however we will do all that using Google Cloud ML Engine.\n",
    "\n",
    "Twitter data is extracted using [this](https://dev.twitter.com/overview/api) api. The data contains tweets posted by the following six Twitter accounts: `realDonaldTrump, mike_pence, GOP, HillaryClinton, timkaine, TheDemocrats`\n",
    "\n",
    "For every tweet, there are two pieces of information:\n",
    "- `screen_name`: the Twitter handle of the user tweeting and\n",
    "- `text`: the content of the tweet.\n",
    "\n",
    "The tweets have been divided into two parts - train and test available to you in CSV files. For train, both the `screen_name` and `text` attributes were provided but for test, `screen_name` is hidden.\n",
    "\n",
    "The overarching goal of the problem is to \"predict\" the political inclination (Republican/Democratic) of the Twitter user from one of his/her tweets. The ground truth (i.e., true class labels) is determined from the `screen_name` of the tweet as follows\n",
    "- `realDonaldTrump, mike_pence, GOP` are Republicans\n",
    "- `HillaryClinton, timkaine, TheDemocrats` are Democrats\n",
    "\n",
    "Thus, this is a binary classification problem.\n",
    "\n",
    "The code to create features from data and to create labels is provided below, you do not need to re-write it. Run the following cells to generate data in a [format](https://cloud.google.com/ml-engine/docs/algorithms/preprocessing-data) that the ML Engine can understand.\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\"> DO NOT change this code or use code form *HW3*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(processed_tweets):\n",
    "    return (~processed_tweets['screen_name'].isin(['realDonaldTrump', 'mike_pence', 'GOP'])).astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_traintest():\n",
    "    # Setup training and testing paths\n",
    "    train_path = os.path.join('data', 'tweets_train.csv')\n",
    "    train_gcinput_path = os.path.join('data', 'train_gcinput.csv')\n",
    "\n",
    "    test_path = os.path.join('data', 'tweets_test.csv')\n",
    "    test_gcinput_path = os.path.join('data', 'test_gcinput.json')\n",
    "\n",
    "    # Setup vectorizer and PCA\n",
    "    eng_stopwords = set(stopwords.words('english'))\n",
    "    vec = TfidfVectorizer(stop_words=eng_stopwords, max_df=0.8, min_df=3, max_features=5000)\n",
    "    pca = PCA(n_components=300)\n",
    "\n",
    "\n",
    "    # Setup training data\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    train_feat = vec.fit_transform(train_df['text'])\n",
    "    train_feat = pca.fit_transform(train_feat.todense())\n",
    "    train_lab = create_labels(train_df)\n",
    "    df_feat = pd.DataFrame(train_feat)\n",
    "    df_lab = pd.DataFrame(train_lab)\n",
    "    df = pd.concat([df_lab, df_feat], axis=1)\n",
    "    df.to_csv(train_gcinput_path, index=False, header=False)\n",
    "    # Setup testing data\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    test_feat = vec.transform(test_df['text']).todense()\n",
    "    test_feat = pca.transform(test_feat)\n",
    "    df = pd.DataFrame(test_feat)\n",
    "    def online_prediction_format(row):\n",
    "        key = row.name\n",
    "        value = str(row.values.tolist()).replace(\"[\", \"\").replace(\"]\",\"\")\n",
    "        instance = {'key':str(key), 'csv_row':value}\n",
    "        return instance\n",
    "    df = df.apply(lambda x: online_prediction_format(x), axis=1)\n",
    "    df.to_json(test_gcinput_path,orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_traintest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code should produce two files `train_gcinput.csv` and `test_gcinput.json` in the `data` subdirectory. `train_gcinput.csv` contains labels in first column and features in remaining columns for each record and `test_gcinput.json` just contains the features and unique key for each record as a JSON instance required by the [prediction](https://cloud.google.com/ai-platform/prediction/docs/overview?hl=en_US) service. There are 300 features for each record.\n",
    "\n",
    "**Side Note:** PCA is a [dimensionality reduction algorithm](https://medium.com/@mayur87545/dimensionality-reduction-1663f960293f) which makes our file size smaller and our computations simpler. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Data to Cloud\n",
    "\n",
    "Before we can train our algorithm, we need to upload `train_gcinput.csv` to the cloud so that the ML engine can access it. But how do we do that?\n",
    "\n",
    "In *Part 0*, we created a bucket which makes it easier to store files on the cloud and enables different Google Cloud services to access those files. You can think of buckets as folders and all the services on cloud as applications which have access to these folders.\n",
    "\n",
    "\n",
    "Go to [Google Cloud Console](https://console.cloud.google.com/) and you should be able to see *Storage* under *Resources* card on project *Dashboard*. Click on it and it will take you to *Storage Browser*. Select your bucket and upload `train_gcinput.csv` in the root of the bucket. Creat a new subdirectory in the root of the bucket and name it as `output`, we will use it later to store logs and trained model. It should like the following figure:\n",
    "\n",
    "<img src=\"./screens/mlengine/01-upload-files.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "Now, we have all the files necessary to train our model. Go to [Google Cloud Console](https://console.cloud.google.com) and from the sidebar, select *ML Engine > Jobs* as shown in the following figure.\n",
    "\n",
    "<img height=\"500\" width=\"500\" src=\"./screens/mlengine/02-select-jobs.png\" />\n",
    "\n",
    "\n",
    "If you have not used *Jobs* before then you first need to Enable API to use this service. Click *ENABLE API* (it might take a while).\n",
    "\n",
    "<img src=\"./screens/mlengine/03-enable-api.png\" />\n",
    "\n",
    "Google cloud provides some built-in models which can be trained on any dataset. Once, the API is enabled, click on *NEW TRAINING JOB* and select *Built-in algorithm training* from the dropdown as shown below:\n",
    "\n",
    "\n",
    "<img src=\"./screens/mlengine/04-builtin.png\" />\n",
    "\n",
    "We will use a built-in linear classifier for training. Select *Linear Learner* as your training algorithm and click *NEXT*\n",
    "\n",
    "<img src=\"./screens/mlengine/05-select-model.png\" />\n",
    "\n",
    "Click *BROWSE* and select `train_gcinput.csv` from the bucket under *Training data path* and setup the remaining arguments as shown in the following screenshot. The validation data is used while training the model for hyper-parameter tuning and we will also use 10% of the training data as Test data for model evaluation. Note that this is not the final test data that we will be using for predictions. Click *NEXT*.\n",
    "\n",
    "<img src=\"./screens/mlengine/06-select-data.png\" />\n",
    "\n",
    "Select model type as classification and leave everything as is, click *NEXT*.\n",
    "\n",
    "\n",
    "<img src=\"./screens/mlengine/07-arguments.png\" />\n",
    "\n",
    "In the job setup, you can specify a *Job ID* and the [type of machine](https://cloud.google.com/ml-engine/docs/tensorflow/machine-types) you want to run this on. We will run this job on a standard_gpu machine. Note that there are other high-end machines available as well. Click *DONE*.\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\"> High-end machines may incur high computational cost and built-in models do not have the capacity to fully utilize them. [Details](https://cloud.google.com/ml-engine/docs/algorithms/linear-learner).\n",
    "\n",
    "<img src=\"./screens/mlengine/08-job-setup.png\" />\n",
    "\n",
    "\n",
    "Your job should be up and running now and you should be able to see a list of running jobs:\n",
    "\n",
    "<img src=\"./screens/mlengine/09-job-list.png\" />\n",
    "\n",
    "You can open a job by clicking on Job ID. This will take you to the job progress page (shown below) where you can mointor resources being used by the job, job logs, parameters and progress. Click on View Logs to view and stream the logs, if something goes wrong then you should be able to see it in the logs.\n",
    "\n",
    "<img src=\"./screens/mlengine/10-job-progress.png\" />\n",
    "\n",
    "When the job is completed you should be able to see the following page. It takes about 12-15 minutes.\n",
    "\n",
    "<img src=\"./screens/mlengine/11-job-completed.png\" width=\"500\" height=\"200\" />\n",
    "\n",
    "Now that our model is trained, we can see the performance of the model using TensorBoard, a toolkit for ML experimentation.\n",
    "\n",
    "<img src=\"./screens/mlengine/15-open-tensorboarda.png\" width=\"500\" height=\"200\" />\n",
    "\n",
    "Click \"RUN IN CLOUD SHELL\" which should open a console as shown in figure below with a command copied into the console.\n",
    "Press Enter and click the \"Web Preview\" button circled in image below.\n",
    "<img src=\"./screens/mlengine/16-open-tensorboardb.jpg\" height=\"500\" weight=\"200\" />\n",
    "\n",
    "This will launch the TensorBoard interface where you can see the performance metrics of the model and the baseline (majority label classifier).\n",
    "\n",
    "<img src=\"./screens/mlengine/17-tensorboard.png\" />\n",
    "\n",
    "Now, we are satisfied with our performance and we can deploy it and make predictions using this model on the cloud. Click *Deploy Model* button in the figure above. Deploy as a new model and specify a name, click *CONFIRM*\n",
    "\n",
    "<img src=\"./screens/mlengine/12-deploy-model.png\" />\n",
    "\n",
    "Specify a version (v1) and click *SAVE*.\n",
    "\n",
    "<img src=\"./screens/mlengine/13-version.png\" />\n",
    "\n",
    "It will take you to your model page, your model is now being deployed, wait for the green check to appear next to version before you proceed.\n",
    "\n",
    "<img src=\"./screens/mlengine/14-model-ready.png\" />\n",
    "\n",
    "Your model is now deployed on the cloud and can be used to make predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "We can either use our local machine to make calls to the Google Cloud and make predictions using gcloud command from [Google Cloud SDK](https://cloud.google.com/sdk/docs/) or we can use [Google Cloud Shell](https://cloud.google.com/shell/) which comes with Google Cloud SDK. We will use *Google Cloud Shell* in this tutorial.\n",
    "\n",
    "Visit [Google Cloud Console](https://console.cloud.google.com) and click on *Activate Cloud Shell* icon in the top bar as shown in the following screenshot. If you have Cloud Shell already open, click \"+\" icon to open a new Shell tab.\n",
    "\n",
    "<img src=\"./screens/cloudshell/01-open-shell.png\" />\n",
    "\n",
    "This should open a terminal in the browser and this is just like a regular linux terminal, you can give a command `ls` to see which files are there. Now, we can upload files to this terminal using the menu on top-right of the terminal window. We need to upload `homework4-key.json` file that we downloaded earlier and `test_gcinput.json` file from `data` directory.\n",
    "\n",
    "<img src=\"./screens/cloudshell/02-upload-files.png\" />\n",
    "\n",
    "Once the files are uploaded you can verify success using `ls` command.\n",
    "\n",
    "<img src=\"./screens/cloudshell/03-list-files.png\" />\n",
    "\n",
    "Now, you can run the following command to enable authentication using the key\n",
    "\n",
    "```\n",
    "export GOOGLE_APPLICATION_CREDENTIALS=homework4-key.json\n",
    "```\n",
    "\n",
    "Once, the authentication is enabled, you can use the following `gcloud` command to make predictions:\n",
    "\n",
    "\n",
    "```\n",
    "gcloud ai-platform predict --model linear_classifier_model --version v1 --json-instances test_gcinput.json\n",
    "```\n",
    "\n",
    "If everything goes right, then you should be able to see the following output. Here, *CLASSES* are just binary class labels 0/1 and *LOGISTIC* shows the probability of being in that class.\n",
    "\n",
    "<img src=\"./screens/cloudshell/04-pred-output.png\" />\n",
    "\n",
    "Let us save these results in JSON format using this command.\n",
    "\n",
    "\n",
    "```\n",
    "gcloud ai-platform predict --model linear_classifier_model --version v1 --json-instances test_gcinput.json --format=json > results.json\n",
    "```\n",
    "\n",
    "This should create a file `results.json` in current directory. You can view the produced JSON using this command:\n",
    "\n",
    "```\n",
    "cat results.json\n",
    "```\n",
    "\n",
    "<img src=\"./icons/save.png\" width=\"20px\" height=\"20px\" align=\"left\"> Select *Download File* from the top-right menu and save `results.json` in your `sub` (submission) subdirectory and that's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Submitting\n",
    "\n",
    "<img src=\"./icons/save.png\" width=\"20px\" height=\"20px\" align=\"left\"> Make sure that you have all the required screenshots and data files in `sub` subdirectory signified by this icon.\n",
    "\n",
    "<img src=\"./icons/edit.png\" width=\"20px\" height=\"20px\" align=\"left\"/> Make sure that you have made all necessary additions to the notebook signified by this icon.\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\"/> Make sure that you have read all the warnings.\n",
    "\n",
    "\n",
    "Finally,\n",
    "\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\"> Once the homework is fully graded, you may remove your project from Google cloud or stop any services to avoid running expenses. Most likely there will be no running expenses even if you do not stop/remove the project but it is usually not recommended to keep unecessary components running on Google Cloud as it may incur additional cost.\n",
    "\n",
    "<img src=\"./icons/warning.png\" width=\"20px\" height=\"20px\" align=\"left\"> Before submitting written part of the HW, make sure that all parts of your solution are completely visible, pay special attention to the images you have included.\n",
    "\n",
    "Now, follow the instructions on top of this notebook to make submissions. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
